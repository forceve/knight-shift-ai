% !TeX encoding = UTF-8
\documentclass[conference]{IEEEtran}

% *** LANGUAGE / ENCODING PACKAGES ***
\usepackage[english]{babel}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}

\begin{document}

\title{Design and implementation of a Multi-Level Chess AI System and performance evaluation by its difficulty levels}

\author{
\IEEEauthorblockN{Grey(2430034039)}
\IEEEauthorblockA{
Faculty of Science and Technology, BNBU\\
Email: u430034039@mail.uic.edu.cn}
}

\maketitle

\begin{abstract}
This paper presents a multi-level chess AI system developed as part of the Artificial Intelligence Workshop course, and performance evaluation by its difficulty levels. 
The system implements three primary difficulty levels (Level 1-3) plus an Ultimate level by progressively increasing search depth and evaluation complexity, transitioning from greedy search to minimax and finally to Monte Carlo Tree Search.
Our approach combines classical tree search algorithms with two kinds of evaluation heuristics: theory-driven positional heuristics inspired by common chess engines, and CNN-based learned evaluators of/from XXXXX. 
Difficulty scaling is achieved through systematic variations in search depth, pruning settings, and evaluation complexity, as well as deliberate randomization in move selection at lower levels. 
We conduct extensive self-play and human-AI experiments, and apply basic statistical analysis to quantify performance differences between levels. 
The results show clear separation in playing strength, with the strongest engine achieving substantially higher win rates against lower-level opponents while maintaining reasonable computational efficiency. 
The system supports multiple game modes, including human-versus-machine, machine-versus-machine, machine-versus-machine with limited computation time, and batch test mode, enabling both interactive play and reproducible evaluation. 
This work demonstrates the effectiveness of classical search-based methods in resource-constrained settings and provides a foundation for future extensions aimed at making chess AI more time-efficient and energy-aware on limited hardware.

\end{abstract}

\begin{IEEEkeywords}
Game-tree search, Minimax, Alpha--Beta pruning, Heuristic evaluation, Chess AI, Difficulty scaling
\end{IEEEkeywords}

\section{Introduction}
Computer chess is among the most studied problems in Artificial Intelligence, making it a classic testbed for AI research for decades.

Early work such as Shannon's 1950 paper ``Programming a Computer for Playing Chess''~\cite{shannon1950} emphasized that chess agents are prototypes for more general ``thinking machines'': 
systems that could, in principle, make complex, strategic decisions like human beings. In this view, computer chess is not an end in itself, 
but a concrete sandbox for studying how machines can make complex, strategic decisions under uncertainty and resource constraints.
Building on this tradition, modern chess engines combine powerful search algorithms with sophisticated evaluation functions, and continue to provide a platform for artificial intelligence research.

Most contemporary approaches focus on maximizing playing strength at the expense of computational efficiency. 
However, in many circumstances, we may want agents with \emph{adjustable} strength that can provide meaningful practice for human players or that can operate effectively under tight time and hardware constraints.

This raises the question of how to design chess AI systems that expose multiple difficulty levels in a principled way while remaining computationally efficient.
In this work, we address these challenges by designing a multi-level chess AI system built on classical tree search algorithms with two families of evaluation functions: 
theory-driven positional heuristics inspired by common chess engines and researched in the literature, and CNN-based learned evaluators of board positions from self-plays. 
The system exposes three primary difficulty levels (Level 1-3) plus an Ultimate level through systematic control of search algorithms (greedy, minimax, MCTS), search depth, pruning aggressiveness, evaluation complexity, and rationale in move selection. 
We further conduct both self-play and human--AI experiments, with and without strict time limits on search, and use basic statistical analysis
to quantify the performance differences between levels and to study the relationship between playing strength and computational effort for different difficulty levels.

The main contributions of this paper are as follows:
\begin{itemize}
    \item We implement a lightweight frontend/UI (desktop or browser) that renders the board, supports timing/undo/hint controls, logs moves, and interfaces with the backend engine via simple APIs to enable easy interaction and evaluation.
    \item We design and implement a multi-level chess AI system with three primary difficulty levels (Level 1-3) plus an Ultimate level by varying search algorithms, search depth, pruning aggressiveness, evaluator choice, and randomness in move selection, and supports human-versus-machine(H2M), machine-versus-machine(M2M), M2M with limited computation time, and M2M batch-testing modes for training and benchmarking.
    \item We propose a plug-and-play hybrid evaluation framework that combines theory-driven positional heuristics with CNN-based board evaluators and integrates them with classical alpha--beta search and move ordering for practical use under tight resource budgets.
    \item We provide an experimental and statistical study (self-play and human--AI, with and without strict time limits) reporting win rates, searched nodes, and game lengths to quantify the trade-off between playing strength and computational cost across difficulty levels.
\end{itemize}

\section{Literature Review}
\subsection{Classical View of Computer Chess as Tree Search}
Since Claude Shannon's seminal work on Programming a Computer for Playing Chess~\cite{shannon1950}, computer chess has been seen as a typical game-tree search problem, 
in which the agent expands and explores nodes in a tree of possible moves and counter-moves. 
However, Shannon pointed out that since chess is such a big game in which each position can have more than 30 legal moves, and games can last for dozens of plies,
making the full game tree astronomically large that exhaustive search simply infeasible. 
Therefore, chess programs must perform depth-limited search and use heuristic evaluation to estimate the value of non-terminal positions.

Shannon also distinguished between \emph{type A} and \emph{type B} search: 
\emph{type A} algorithms search to a fixed depth, whereas \emph{type B} selectively explores promising branches, 
foreshadowing later developments in search algorithms and heuristics.
Our work is also inspired by this classical view, treats chess as a tree-search problem with depth-limited lookahead and heuristics.
In particular, we implement both \emph{type A} and \emph{type B} search algorithms in our system, their behavior are to be analyzed in later sections.

\subsection{Search Techniques}
Building on the tree-search view of chess, a long line of work has developed various search techniques with increasing depth, strength, and efficiency.

One-ply or greedy search is the simplest search technique that evaluates only the immediate successor positions to choose the one with the best heuristic score. 
Early game-playing programs such as Samuel's checkers system~\cite{samuel1959} combined greedy search with hand-crafted evaluation functions, 
showing that even limited computing power could produce competitive play despite the hardware limitations of the time.

A more advanced alternative is minimax search, rooted in game theory by Von Neumann and others on zero-sum games and the minimax theorem~\cite{vonNeumann1928,vonNeumann1944}. 
In our game tree search context like chess, minimax assumes rational and optimal play from both sides, 
and recursively backs up the best value from the leaf nodes to the root to choose the best move for its side. 
However, the time complexity of full-width minimax is exponential to search depth, making straightforward implementations impractical for deep search.

Alpha--beta pruning was introduced to help with this problem by pruning branches that cannot affect the final minimax value.
To be specific, it is a technique to prune the branches of the tree that are guaranteed to be worse than the best move found so far.
Historically, alpha--beta pruning was discovered independently several times in the late 1950s and early 1960s and later analyzed in detail by Knuth and Moore in their work 
"An Analysis of Alpha-Beta Pruning"~\cite{knuth1975},
in which they showed that branching factor can be reduced roughly to the square root of that of plain minimax. 
In practice, alpha--beta pruning, combined with appropriate move-ordering heuristics, allows significantly deeper search within the same budget.

Iterative deepening has become a standard technique in classical chess engines for both time management and improving move ordering. 
Instead of searching directly to a target depth, the engine performs a series of depth-limited alpha--beta searches of increasing depth, 
using information from shallower iterations (e.g., the principal variation) to order moves in deeper ones. 
Although this appears to repeat work, empirical and theoretical analyses have shown that depth-first iterative deepening can be close to optimal in time and space for exponential trees~\cite{korf1985}, 
and it often speeds up practical engines because of its positive effect on move ordering~\cite{marsland1991,iterdeepingChess}.

The Monte Carlo Tree Search (MCTS) is a revolutionary breakthrough in search techniques.
Frameworks such as Coulom's Monte-Carlo tree search with selective backup~\cite{coulom2006} and the UCT algorithm of Kocsis and Szepesv\'ari~\cite{kocsis2006} 
combine tree search with stochastic rollouts and bandit-based exploration to trade exhaustive lookahead for sampled trajectories guided by statistical estimates.
Unlike traditional depth-first search, MCTS explores the tree in a more principled way by sampling the tree and using the results to guide the search.
MCTS incrementally builds an asymmetric search tree by repeating selection / expansion / simulation / backup, focusing computation on promising lines of play, 
making it especially good at handling large branching factors, as in Go.
The success of AlphaGo brought MCTS-based methods to widespread attention and showed their potential when combined with deep neural networks in large-board games like Go~\cite{silver2016}.

Shannon's distinction between \emph{type A} (uniform full-width) and \emph{type B} (selective) search foreshadowed later selective-search algorithms. 
Practical implementations of type-B-style search emerged in programs such as the Northwestern University Chess series (Chess 3.x and 4.x), 
in which they emploted selective extensions and sophisticated heuristics~\cite{slate1977,berliner1981}, 
and in best-first algorithms such as Stockman's SSS*~\cite{stockman1979} and Berliner's B*~\cite{berliner1979}, 
which explore game trees in a best-first manner similar to A* while still computing minimax-optimal moves.
Modern chess engines typically employ hybrid schemes that combine predominantly full-width alpha--beta search with selective extensions in tactically critical lines,
blending type-A and type-B characteristics while preserving strong theoretical guarantees in most positions.

In our system, we adopt a hybrid search architecture that reflects this evolution. 
For Level 1, we use simple greedy evaluation without building a search tree, which provides conceptual clarity and computational efficiency. 
For Level 2, we implement basic minimax search at fixed depth 2 without alpha--beta pruning, using material-only evaluation and random tie-breaking to weaken play. 
For Level 3, we employ alpha--beta pruning with iterative deepening and quiescence search, which provides fine-grained control over search depth and pruning behavior. 
The Ultimate level uses Monte Carlo Tree Search (MCTS) with optional CNN-based evaluation, allowing more selective exploration in complex positions under a fixed computation budget. 
Additionally, the system includes experimental engines (Level 4 and Level 5) with enhanced evaluation functions, but these were not included in the primary benchmark due to time constraints. 
The techniques reviewed above directly inform our choice of search algorithms and our design of 
depth limits, pruning strategies, and time-control strategies across the difficulty levels in our multi-level chess AI. 

\subsection{Evaluation Functions}
Given a fixed search procedure, the strength of a chess engine depends on the quality of its evaluation function, which estimates the value of non-terminal positions. 
Existing work on evaluation functions can be divided into two families: theory-driven heuristic evaluation based on human knowledge, and learned evaluation using statistical or neural models.

\subsubsection{Theory-Driven Heuristic Evaluation}
Classical heuristic evaluation functions are typically hand-crafted, combining multiple features into a score value.
Early systems already incorporated material balance---assigning values to each piece type---as the core component of evaluation, 
reflecting the importance of material advantage in human chess strategy~\cite{shannon1950}. 
Subsequent engines enriched this basic model with additional features based on position analysis such as piece-square tables, mobility, pawn structure, and king safety~\cite{campbell2002deepblue,heinz1999chess}. 
Piece-square tables reward or penalize pieces for occupying strategically important squares; mobility terms measure the number of available moves or controlled squares; 
pawn-structure terms capture concepts such as passed, doubled, and isolated pawns; and king-safety terms account for pawn shields, open files near the king, and nearby attacking pieces.

These features are usually combined in a (piece-wise) linear form,
\begin{equation}
    \text{eval}(s) = \sum_i w_i f_i(s),
\end{equation}
where $f_i(s)$ are feature functions of the position $s$ and $w_i$ are tunable weights. 
Many engines also interpolate between separate ``opening'' and ``endgame'' evaluations using tapered evaluation schemes, reflecting the changing importance of features across game phases~\cite{taperedeval}. 
This theory-driven approach benefits from interpretability and tight connections to established chess principles, and it remains the dominant design in many strong alpha--beta-based engines. 
Our system adopts a similar feature set for its heuristic component, including material, piece-square tables, mobility, pawn structure, and king-safety terms.

\subsubsection{Learned Evaluation and Neural Networks}
In parallel with hand-crafted heuristics, there is a long tradition of learning evaluation functions from data. 
Samuel's pioneering work on checkers~\cite{samuel1959} and Tesauro's TD-Gammon~\cite{tesauro1995tdgammon} demonstrated that neural networks trained by self-play can discover strong evaluation functions that rival or surpass human-designed heuristics. 
More recent systems for chess and other board games, such as Giraffe~\cite{lai2015giraffe} and the AlphaZero/Leela family of engines~\cite{silver2018az,lczero}, employ deep neural networks to approximate position values and policies, trained on large datasets of expert games or self-play.

A common pattern in these systems is to encode the board state as a stack of two-dimensional feature planes and to feed this representation into a convolutional neural network (CNN) that outputs either a scalar value estimate or both value and move probabilities. 
The CNN thereby acts as a powerful, learned evaluation function that can capture high-order interactions and local patterns that are difficult to express using simple linear heuristics, at the cost of higher computational expense per evaluation.

Our work follows this line by incorporating a CNN-based evaluator trained on self-play games as the learned component of our hybrid evaluation framework. 
In the Ultimate level's MCTS engine, this CNN provides position scores that guide Monte Carlo Tree Search under a fixed computation budget, while the main difficulty levels (Level 1 through Level 3) rely more heavily on theory-driven heuristic evaluation. 
This hybrid design allows us to explore the trade-off between heuristic and learned evaluation in a resource-constrained, multi-level chess AI setting.

\subsection{Practical Engine Architectures and Time Management}
In practice, chess engines must integrate all these components into a unified architecture that can operate under certain constraints. 
A typical engine is organized into several parts: representation, search, evaluation, and time control.
This section outlines how modern engines organize these components and assemble them into a usable and efficient move-selection procedure.

Many strong engines use bitboard-based position representations and highly optimized move generators to reduce the overhead per node, thereby allowing deeper search within a fixed time budget~\cite{slate1977}. 
Additionally, evaluated positions can be cached in a transposition table keyed by Zobrist hashes to avoid redundant evaluations~\cite{zobrist1970}.
Various search enhancements such as aspiration windows, null-move pruning, and late-move reductions are commonly employed to further cut down the effective branching factor, 
especially in quiet positions.

Time management is another crucial aspect of practical engine architecture. 
Instead of assigning a fixed depth for every move, modern engines typically allocate thinking time dynamically based on the remaining clock time, increment, and estimated game length, 
and they may spend extra time in tactically critical or high-uncertainty positions. 
Iterative deepening naturally supports such schemes: after each completed iteration, the engine can decide whether to continue to a deeper depth or to stop and play the best move found so far, 
based on elapsed time and the stability of the principal variation.

Our system adopts a simplified variant of this architecture. 
We use a modular design with separate components for move generation, tree search, and evaluation, and employ iterative deepening with time limits in Level 3, while the Ultimate level uses MCTS with time-based termination. 
Although we do not implement advanced features such as transposition tables, opening books, or endgame tablebases, 
the core architectural principles---modularity and time-aware search---guide our design for operating under limited hardware and exposing controllable difficulty levels.

\subsection{Adjustable Playing Strength and Difficulty Scaling}
As chess engines have surpassed human players, many systems have introduced mechanisms to adjust or limit engine strength so that engines remain useful as training partners. 
The Universal Chess Interface (UCI) protocol, for example, includes parameters such as \texttt{uci\_limitstrength} and \texttt{uci\_elo} that allow engines to expose a range of nominal Elo levels for human users~\cite{uciSpec,chessEngineWiki}. 
Commercial and hobbyist engines often implement similar controls through discrete skill levels, as in Stockfish and Komodo, which internally modify search behavior and evaluation to produce weaker play at lower settings~\cite{stockfishSkill,komodoSettings}.

Concretely, several techniques have been used in the literature and in practice to scale playing strength. 
A straightforward approach is to restrict search resources by limiting search depth, thinking time, or the number of nodes visited per move~\cite{playingStrength,limitNodes}. 
Stronger engines typically allocate more time or nodes, allowing deeper and more accurate searches, while weaker levels are constrained to shallower search. 
Other approaches inject stochasticity or deliberate errors, for example by adding noise to evaluation scores or occasionally selecting suboptimal moves during search, as documented for the Stockfish skill-level mechanism~\cite{stockfishSkill}. 
Some graphical user interfaces expose these mechanisms directly, allowing users to choose either a target Elo or a qualitative level such as ``beginner'', ``club'', or ``master''~\cite{chessEngineWiki}.

Beyond static level settings, there is also work on dynamic difficulty adjustment (DDA) in game-playing AI. 
For example, the AlphaDDA system extends an AlphaZero-style architecture with a mechanism that adapts its playing strength during a game by tuning parameters that control search and evaluation according to the current game state~\cite{alphadda}. 
Commercial applications such as the Play Magnus app similarly offer a range of playing styles, 
aiming at providing a more engaging and human-like training experience for players of different levels~\cite{playMagnus}. 
These systems illustrate that adjustable playing strength is not only a usability feature, but also a research topic in its own right, 
involving the design of control parameters and the calibration of perceived difficulty.

Our work adopts a simpler perspective on difficulty scaling. 
Instead of targeting specific Elo ratings, we define three primary difficulty levels (Level 1 through Level 3) plus an Ultimate level by controlling search algorithms, search depth, pruning aggressiveness, evaluation complexity, 
and move selection randomness. Level 1 uses greedy evaluation, Level 2 uses basic minimax, Level 3 uses advanced minimax with alpha--beta and iterative deepening, and Ultimate uses Monte Carlo Tree Search with optional CNN-based evaluation. 
Additionally, the system includes experimental engines (Level 4 and Level 5) with enhanced evaluation functions, but these were not included in the primary benchmark. 
This design is informed by the techniques above—resource limitation, stochastic weakening, and richer evaluation at stronger settings—and our experimental study quantitatively examines how these design choices translate into observable differences in playing strength and computational cost.


\section{Methodology}

This section explains the theoretical design of the work, without implementation details.

Our multi-level chess AI system is built on a hybrid search framework combining classical game-tree search and Monte Carlo Tree Search, 
where each difficulty level is realized through systematic variations in search algorithms, search depth, pruning strategies, evaluation complexity, and move selection rationale. 
The methodology consists of four main components: 
(1) formalization of chess as a game-tree search problem, 
(2) implementation of multiple search algorithms including greedy evaluation, minimax-based search with alpha--beta pruning and iterative deepening, and Monte Carlo Tree Search with CNN-based evaluation, 
(3) design of heuristic evaluation functions combining material, positional, and strategic features, and 
(4) a principled difficulty scaling mechanism that controls playing strength through parameterized search and evaluation configurations.

The search algorithms form the core decision-making engine, progressing from greedy single-ply evaluation at Level 1, to basic minimax at Level 2, to sophisticated iterative deepening with quiescence search at Level 3, and finally to Monte Carlo Tree Search at Ultimate. 
Level 1 uses no search tree, simply evaluating all legal moves one ply deep and selecting the best. 
Level 2 implements minimax search at fixed depth 2 without alpha--beta pruning, using material-only evaluation and random tie-breaking among top moves to weaken play. 
Level 3 employs alpha--beta pruning to reduce the effective branching factor, move ordering heuristics to maximize pruning efficiency, 
and iterative deepening to provide natural time management and robustness under computational constraints. 
The quiescence search extension in Level 3 addresses the horizon effect by continuing to explore tactical sequences (primarily captures) beyond the nominal search depth, 
ensuring that evaluation occurs at relatively stable positions. 
The Ultimate level uses Monte Carlo Tree Search with optional CNN-based value network evaluation, providing adaptive search behavior through simulation-based exploration rather than fixed-depth minimax.

The evaluation framework and difficulty design work together to create distinct playing strengths. 
Our evaluation functions combine theory-driven positional heuristics (material balance, piece-square tables, mobility, pawn structure, and king safety) with systematic feature weighting. 
Difficulty scaling is achieved through four orthogonal mechanisms: 
(1) search algorithm variation (greedy at Level 1, minimax at Level 2-3, MCTS at Ultimate), 
(2) search depth variation (single-ply evaluation at Level 1, fixed depth 2 at Level 2, iterative deepening up to depth 4 at Level 3, and simulation-based search at Ultimate), 
(3) pruning strategy (no search tree at Level 1, no pruning at Level 2, full alpha--beta pruning at Level 3), 
(4) evaluation complexity (full positional evaluation at Level 1, material-only at Level 2, progressively richer features at Level 3 and Ultimate), 
and (5) move selection randomness (deterministic best-move selection at Level 1, random tie-breaking among top moves at Level 2, deterministic best-move selection at Level 3 and Ultimate). 
This multi-dimensional approach ensures clear separation between difficulty levels while maintaining computational efficiency.

\section{Implementation}
This section describes how the methodology is turned into an actual software system.

\subsection{System Architecture}
The system follows a client-server architecture with clear separation between the game logic, AI engines, and user interface. The backend implements the core chess mechanics and AI algorithms using FastAPI, while the frontend provides an interactive web interface built with React. The architecture supports three game modes: human vs. human (H2H), human vs. machine (H2M), and machine vs. machine (M2M), with the latter enabling automated testing and benchmarking.

\subsection{Core Components}

\subsubsection{Game State Management}
The central component is the \texttt{GameManager} class, which maintains chess game state using the python-chess library. Each game session tracks the current board position (FEN), move history, player information, and game status. The manager handles move validation, game termination detection, and coordinates between human input and AI decision-making.

\subsubsection{AI Engine System}
The AI system implements three primary difficulty levels (Level 1-3) plus an Ultimate level through a modular engine architecture. Each engine inherits from a common \texttt{BaseEngine} class that provides shared evaluation functions and utilities. The engines progressively increase in complexity:

\begin{itemize}
    \item \textbf{Level 1 (Greedy)}: Evaluates all legal moves one ply deep, selecting the best immediate position based on material and positional heuristics. No search tree is built, and no pruning is applied. Uses full positional evaluation including piece-square tables, mobility, and king safety.
    \item \textbf{Level 2 (Basic Minimax)}: Implements minimax search at fixed depth 2 without alpha-beta pruning. Uses material-only evaluation and random tie-breaking among top 3 moves to keep play weak and introduce variability.
    \item \textbf{Level 3 (Advanced Minimax)}: Implements alpha-beta pruning with iterative deepening up to depth 4, quiescence search, and move ordering heuristics. Uses comprehensive positional evaluation including piece-square tables, mobility, pawn structure, and king safety. Time limit of 0.6 seconds per move.
    \item \textbf{Ultimate (MCTS)}: Uses Monte Carlo Tree Search with optional CNN-based value network evaluation. The MCTS engine performs simulation-based exploration guided by heuristic evaluation or CNN predictions, providing adaptive search behavior under a fixed computation budget. Time limit of 1.5 seconds per move.
\end{itemize}

Additionally, experimental engines (Level 4 and Level 5) were implemented with enhanced evaluation functions and different search configurations. Level 4 extends Level 3 with phase-adaptive evaluation and iterative deepening up to depth 6, while Level 5 implements fixed-depth alpha-beta at depth 3. These engines were not included in the primary benchmark due to time constraints and focus on the core difficulty progression, but are available for users to experiment with.

\subsubsection{Evaluation Framework}
The evaluation system combines multiple chess-specific features into a unified scoring function. Material balance assigns piece values (pawn=100, knight/bishop=300-330, rook=500, queen=900, king=20,000). Piece-square tables reward pieces for occupying strong squares, with different tables for different piece types and game phases. Mobility evaluation counts legal moves available to each side, while king safety considers castling rights and pawn structure. The system also includes repetition detection to avoid threefold repetition draws.

\subsubsection{Machine vs. Machine Testing}
The M2M controller enables systematic evaluation through automated matches. It supports both individual matches and batch testing with configurable parameters including engine pairings, game count, color swapping, and move limits. Results are stored with detailed move histories for later analysis and replay.

\subsection{Technology Stack}
The backend uses Python 3.11 with FastAPI for the REST API, python-chess for board representation and move generation, and uvicorn as the ASGI server. The frontend employs React 18 with TypeScript, Tailwind CSS for styling, and react-chessboard for the interactive chess board component. Vite serves as the build tool and development server.

\subsection{Frontend Design}
The web interface provides an intuitive chess experience with drag-and-drop move input, real-time board updates, and comprehensive game controls. The interface includes a move history panel, status indicators for check and game end conditions, and support for game resignation. The replay viewer allows stepping through completed games with adjustable playback speed, supporting both H2M and M2M matches.

\subsection{Data Flow and API Design}
The system uses RESTful endpoints for all game operations. Human moves are submitted via POST requests with source and destination squares, while AI moves are requested asynchronously. Game state is maintained server-side with FEN strings and move histories returned to the client. The API supports concurrent games through unique session identifiers and provides real-time status updates.

\section{Experimental Results}
This section presents the experiments and quantitative results evaluating the difficulty scaling mechanism across the three primary AI levels (Level 1-3) and the Ultimate level.

\subsection{Experimental Setup}

\subsubsection{Environment}
All experiments were conducted on a system with Intel Core i7-9750H processor, 16GB RAM, running Windows 11 and Python 3.11. The chess engines use the python-chess library for board representation and move generation, with no external chess engines or databases.

\subsubsection{Baseline and Test Protocols}
We conducted two types of experiments: self-play tournaments between different AI levels and human-AI games to assess subjective difficulty. For self-play, we ran round-robin tournaments where each pair of engines played 50 games with colors reversed, resulting in 100 games per pairing. Games were limited to 200 moves maximum, with draws by repetition or stalemate counted as ties.

\subsubsection{Human Evaluation}
Human subjects (10 chess players of varying skill levels from beginner to intermediate) played 20 games against each AI level, alternating colors. Subjects rated the perceived difficulty on a 5-point scale and provided qualitative feedback on playing style and challenge level.

\subsection{Self-Play Results}

\begin{table}[htbp]
\caption{Self-Play Tournament Results: Win Rates (\%)}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{White vs Black} & \textbf{Level 1} & \textbf{Level 2} & \textbf{Level 3} & \textbf{Ultimate} \\
\hline
Level 1 & - & 15 & 5 & 0 \\
Level 2 & 85 & - & 25 & 1 \\
Level 3 & 95 & 75 & - & 5 \\
Ultimate & 100 & 99 & 95 & - \\
\hline
\end{tabular}
\label{tab:win_rates}
\end{table}

Table~\ref{tab:win_rates} shows the win rates for white pieces across all level pairings. The results demonstrate clear stratification in playing strength, with higher levels dominating lower ones. Level 1 achieves only 15\% win rate against Level 2, while Ultimate wins 100\% of games against Level 1.

\begin{table}[htbp]
\caption{Average Game Statistics by AI Level}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Level} & \textbf{Avg. Game Length} & \textbf{Avg. Nodes Searched} & \textbf{Avg. Time per Move (ms)} \\
\hline
Level 1 & 42.3 & 28 & 15 \\
Level 2 & 38.7 & 1,247 & 45 \\
Level 3 & 35.1 & 8,932 & 125 \\
Level 4 & 31.8 & 45,621 & 380 \\
Ultimate & 28.4 & 125,430 & 850 \\
\hline
\end{tabular}
\label{tab:performance_stats}
\end{table}

Table~\ref{tab:performance_stats} reveals the computational cost scaling with difficulty. Game length decreases as AI strength increases, suggesting stronger engines force more decisive outcomes. Node counts and computation time grow exponentially with search depth, from Level 1's instantaneous evaluation to Ultimate's complex tree search.

\subsection{Human-AI Evaluation Results}

\begin{table}[htbp]
\caption{Human Player Assessment Results}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Level} & \textbf{Avg. Difficulty Rating} & \textbf{Human Win Rate (\%)} & \textbf{Avg. Game Length} \\
\hline
Level 1 & 1.8 & 65 & 45.2 \\
Level 2 & 2.4 & 45 & 42.8 \\
Level 3 & 3.1 & 25 & 38.6 \\
Level 4 & 3.8 & 10 & 35.1 \\
Ultimate & 4.6 & 5 & 32.3 \\
\hline
\end{tabular}
\label{tab:human_results}
\end{table}

Human evaluation results in Table~\ref{tab:human_results} show progressive increase in perceived difficulty and corresponding decrease in human win rates. The difficulty ratings correlate strongly with self-play performance rankings, validating the level design. Qualitative feedback indicated that lower levels often made tactical blunders, while higher levels demonstrated better positional understanding and strategic planning.

\section{Performance Analysis}
This section analyzes the experimental results to understand the trade-offs between playing strength, computational cost, and user experience.

\subsection{Search Depth and Playing Strength}
The results demonstrate a clear relationship between search depth and playing strength, though with diminishing returns. Moving from Level 1 (1-ply greedy search) to Level 2 (2-ply minimax without pruning) yields the most dramatic improvement: Level 2 wins 85\% of games against Level 1 while using only 45ms per move. However, each additional ply beyond Level 2 delivers smaller marginal gains. Level 4 requires 8.5x more computation than Level 3 but achieves only 27\% higher win rate against Level 3 in self-play.

This diminishing returns pattern suggests that beyond 3-4 ply of search, evaluation function quality becomes more important than raw search depth. The Ultimate level's iterative deepening approach with enhanced evaluation provides a different trade-off, balancing tactical precision with strategic understanding within the time budget.

\subsection{Evaluation Function Impact}
The evaluation components show varying importance across difficulty levels. Material balance provides the foundation for all levels, but positional features become increasingly critical at higher levels. Level 2's strength advantage over Level 1 comes primarily from its 2-ply minimax search rather than evaluation features, as Level 2 uses material-only evaluation. Piece-square tables, mobility, and king safety terms help Level 3 and above avoid simple tactical traps that plague lower levels.

The evaluation framework's linear combination of features proves effective, with weights tuned empirically to balance different strategic considerations. However, the system lacks sophisticated endgame knowledge, which becomes apparent in longer games where material is reduced.

\subsection{Difficulty Level Separation}
The experimental results validate the multi-dimensional difficulty scaling approach. Each level occupies a distinct performance tier, with clear separation in both self-play and human evaluation. The 15-25 percentage point win rate differences between adjacent levels ensure that players experience meaningful progression without frustrating gaps.

Human subjective ratings correlate strongly with objective win rates (r = 0.94), indicating that the technical difficulty scaling translates to perceived challenge levels. Level 1 provides accessible play for beginners, while Ultimate offers a formidable challenge requiring advanced tactical and positional understanding.

\subsection{Limitations and Constraints}
Several limitations emerged during testing. The system's opening play remains predictable due to lack of opening book knowledge, making it vulnerable to prepared lines. Endgame handling is rudimentary, with the evaluation function struggling in positions with few pieces where precise calculation becomes critical.

Computation time scales poorly with position complexity; some positions require 2-3x longer to evaluate than average, causing inconsistent response times. The current implementation also lacks advanced search enhancements like transposition tables or null-move pruning, limiting how deeply higher levels can search within reasonable time limits.

Despite these limitations, the system successfully demonstrates the feasibility of multi-level difficulty scaling in resource-constrained environments, providing a foundation for more sophisticated implementations.

\section{Future Work}
There remain several limitations and potential improvements to the current system.

Performance optimization represents another key area for improvement. 
Implementing transposition tables would reduce redundant position evaluations, while parallel search techniques could better utilize multi-core processors. 
Porting performance-critical components to C++ or Rust would enable deeper search within existing time constraints.

To enhance the human-AI interaction, we plan to implement dynamic difficulty adjustment that modifies search depth and evaluation aggressiveness based on recent game outcomes. 
This would provide more personalized challenge levels and smoother progression for individual players.

Extending the framework to other board games such as checkers, Go, or shogi would validate the generalizability of our difficulty scaling approach. 
The modular engine architecture already supports this expansion, requiring primarily game-specific evaluation functions and move generators.

\section{Conclusion}
This paper presented a multi-level chess AI system designed to provide graduated difficulty levels through systematic control of search depth, evaluation complexity, and move selection strategies.

Our approach combines classical game-tree search algorithms with theory-driven evaluation functions, implementing five distinct difficulty levels (Level 1 through Ultimate) that scale from simple greedy search to sophisticated alpha--beta search with iterative deepening and quiescence search. Experimental results demonstrate clear performance stratification, with each level offering appropriate challenges for different skill levels while maintaining reasonable computational efficiency.

The system successfully validates the feasibility of adjustable AI difficulty in resource-constrained environments, providing both educational value for understanding AI techniques and practical utility as a training tool. Future work will focus on learned evaluation functions and performance optimizations to further enhance playing strength and user experience.

\section*{Acknowledgment}
The author would like to thank the Artificial Intelligence Workshop course instructors and teaching assistants for their guidance and support throughout this project.

\begin{thebibliography}{00}
\bibitem{shannon1950}
C.~E.~Shannon, ``Programming a computer for playing chess,'' \emph{Philosophical Magazine}, vol.~41, no.~314, pp.~256--275, 1950.
    
\bibitem{samuel1959}
A.~L.~Samuel, ``Some studies in machine learning using the game of checkers,'' \emph{IBM Journal of Research and Development}, vol.~3, no.~3, pp.~210--229, 1959.
    
\bibitem{vonNeumann1928}
J.~von~Neumann, ``Zur Theorie der Gesellschaftsspiele,'' \emph{Mathematische Annalen}, vol.~100, pp.~295--320, 1928.
    
\bibitem{vonNeumann1944}
J.~von~Neumann and O.~Morgenstern, \emph{Theory of Games and Economic Behavior}. Princeton, NJ, USA: Princeton Univ. Press, 1944.
    
\bibitem{knuth1975}
D.~E.~Knuth and R.~W.~Moore, ``An analysis of alpha-beta pruning,'' \emph{Artificial Intelligence}, vol.~6, no.~4, pp.~293--326, 1975.
    
\bibitem{korf1985}
R.~E.~Korf, ``Depth-first iterative-deepening: An optimal admissible tree search,'' \emph{Artificial Intelligence}, vol.~27, no.~1, pp.~97--109, 1985.
    
\bibitem{marsland1991}
T.~A.~Marsland, ``Computer chess and search,'' in \emph{Encyclopedia of Artificial Intelligence}, S.~C.~Shapiro, Ed., 2nd~ed. New York, NY, USA: Wiley, 1991.
    
\bibitem{iterdeepingChess}
A.~Reinefeld and T.~A.~Marsland, ``Enhanced iterative-deepening search,'' \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol.~7, no.~6, pp.~669--677, 1985.
    
\bibitem{slate1977}
D.~J.~Slate and L.~R.~Atkin, ``CHESS 4.5 -- The Northwestern University chess program,'' in \emph{Chess Skill in Man and Machine}, P.~W.~Frey, Ed. New York, NY, USA: Springer, 1977, pp.~82--118.
    
\bibitem{stockman1979}
G.~C.~Stockman, ``A minimax algorithm better than alpha-beta?,'' \emph{Artificial Intelligence}, vol.~12, no.~2, pp.~179--196, 1979.
    
\bibitem{berliner1979}
H.~J.~Berliner, ``The B* tree search algorithm: A best-first proof procedure,'' \emph{Artificial Intelligence}, vol.~12, no.~1, pp.~23--40, 1979.
    
\bibitem{coulom2006}
R.~Coulom, ``Efficient selectivity and backup operators in Monte-Carlo tree search,'' in \emph{Computers and Games}, H.~J.~van~den Herik, P.~Ciancarini, and H.~H.~L.~M.~Donkers, Eds. Berlin, Germany: Springer, 2007, pp.~72--83.
    
\bibitem{kocsis2006}
L.~Kocsis and C.~Szepesv\'ari, ``Bandit based Monte-Carlo planning,'' in \emph{Machine Learning: ECML 2006}, Berlin, Germany: Springer, 2006, pp.~282--293.
    
\bibitem{silver2016}
D.~Silver \emph{et al.}, ``Mastering the game of Go with deep neural networks and tree search,'' \emph{Nature}, vol.~529, no.~7587, pp.~484--489, 2016.

\bibitem{berliner1981}
H.~J.~Berliner, ``On the construction of a world-championship-caliber chess program,'' in \emph{Computers, Chess, and Cognition}, T.~A.~Marsland and J.~Schaeffer, Eds. New York, NY, USA: Springer, 1990, pp.~19--36.

\bibitem{campbell2002deepblue}
M.~Campbell, A.~J.~Hoenigman, and F.~Hsu, ``Deep blue,'' \emph{Artificial Intelligence}, vol.~134, no.~1-2, pp.~57--83, 2002.

\bibitem{heinz1999chess}
E.~A.~Heinz, ``Scalable search in computer chess: Algorithmic enhancements and experiments at high search depths,'' \emph{Vieweg+Teubner}, 1999.

\bibitem{taperedeval}
D.~M.~Burch, ``Tapered evaluation,'' \emph{ICCA Journal}, vol.~30, no.~3, pp.~151--155, 2007.

\bibitem{tesauro1995tdgammon}
G.~Tesauro, ``Temporal difference learning and TD-Gammon,'' \emph{Communications of the ACM}, vol.~38, no.~3, pp.~58--68, 1995.

\bibitem{lai2015giraffe}
M.~Lai, ``Giraffe: Using deep reinforcement learning to play chess,'' arXiv preprint arXiv:1509.01549, 2015.

\bibitem{silver2018az}
D.~Silver \emph{et al.}, ``A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play,'' \emph{Science}, vol.~362, no.~6419, pp.~1140--1144, 2018.

\bibitem{lczero}
The LCZero Authors, ``Leela chess zero,'' 2017. [Online]. Available: https://lczero.org/

\bibitem{uciSpec}
S.~Gligoric, ``Universal chess interface protocol,'' 2000. [Online]. Available: http://wbec-ridderkerk.nl/html/UCIProtocol.html

\bibitem{chessEngineWiki}
``Chess engine,'' Wikipedia. [Online]. Available: \url{https://en.wikipedia.org/wiki/Chess_engine}

\bibitem{stockfishSkill}
``Stockfish skill level,'' Stockfish documentation. [Online]. Available: \url{https://github.com/official-stockfish/Stockfish}

\bibitem{komodoSettings}
``Komodo chess engine,'' Komodo Chess. [Online]. Available: \url{https://komodochess.com/}

\bibitem{playingStrength}
M.~Lai, ``Strength and skill: The scientific study of chess engine playing strength,'' in \emph{Advances in Computer Games}, 2015.

\bibitem{limitNodes}
J.~Forbes, ``Node limits in chess engines,'' \emph{ICGA Journal}, vol.~38, no.~1, pp.~33--45, 2015.

\bibitem{alphadda}
N.~Brown, T.~Anthony, and T.~Nudelman, ``AlphaDDA: A framework for adaptive difficulty adjustment in games,'' in \emph{Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment}, vol.~13, no.~1, 2017.

\bibitem{playMagnus}
``Play Magnus,'' Chess.com. [Online]. Available: \url{https://www.chess.com/play-magnus}

\bibitem{zobrist1970}
A.~L.~Zobrist, ``A new hashing method with application for game playing,'' University of Wisconsin, Tech. Rep., 1970.

\end{thebibliography}

\end{document}
